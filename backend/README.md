#  Sigma AI GPT-J Backend

Backend en Python para integrar GPT-J de EleutherAI con la aplicaci贸n Sigma AI.

##  Instalaci贸n y Configuraci贸n

### Prerrequisitos
- Python 3.8 o superior
- GPU con al menos 24GB VRAM (recomendado)
- 32GB RAM m铆nimo

### 1. Configurar entorno
```bash
# Crear entorno virtual
python -m venv venv

# Activar entorno (Linux/Mac)
source venv/bin/activate

# Activar entorno (Windows)
venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

### 2. Iniciar servidor
```bash
# M茅todo 1: Script autom谩tico
chmod +x start.sh
./start.sh

# M茅todo 2: Manual
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

### 3. Verificar funcionamiento
```bash
# Verificar estado
curl http://localhost:8000/health

# Probar generaci贸n
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Explica qu茅 es la inteligencia artificial"}'
```

##  API Endpoints

### `GET /health`
Verificar estado del servicio
```json
{
  "status": "healthy",
  "model_loaded": true,
  "device": "cuda",
  "gpu_available": true
}
```

### `POST /generate`
Generar texto con GPT-J
```json
{
  "prompt": "Tu prompt aqu铆",
  "max_length": 100,
  "temperature": 0.7,
  "top_p": 0.9
}
```

Respuesta:
```json
{
  "generated_text": "Texto generado...",
  "processing_time": 2.5,
  "model_info": {
    "model": "EleutherAI/gpt-j-6B",
    "device": "cuda",
    "parameters": "6B"
  }
}
```

## 锔 Configuraci贸n

### Variables de entorno
```bash
# Opcional: configurar cache de Hugging Face
export HF_HOME=/path/to/cache

# Configurar GPU (si tienes m煤ltiples)
export CUDA_VISIBLE_DEVICES=0
```

### Optimizaciones de memoria
- El modelo usa `torch.float16` en GPU para reducir memoria
- `low_cpu_mem_usage=True` para carga eficiente
- `device_map="auto"` para distribuci贸n autom谩tica en m煤ltiples GPUs

##  Troubleshooting

### Error de memoria
```bash
# Reducir batch size o usar CPU
# Editar main.py y cambiar device a "cpu"
```

### Modelo no carga
```bash
# Verificar espacio en disco (modelo ~24GB)
df -h

# Limpiar cache si es necesario
rm -rf ~/.cache/huggingface/
```

### Puerto ocupado
```bash
# Cambiar puerto en main.py o usar otro
uvicorn main:app --port 8001
```

##  Monitoreo

### Logs
```bash
# Ver logs en tiempo real
tail -f logs/app.log

# Verificar uso de GPU
nvidia-smi
```

### M茅tricas
- Tiempo de respuesta promedio: 2-5 segundos
- Memoria GPU utilizada: ~20-24GB
- CPU: Variable seg煤n configuraci贸n

##  Despliegue en Producci贸n

### Docker (Recomendado)
```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Consideraciones
- Usar balanceador de carga para m煤ltiples instancias
- Implementar cache Redis para respuestas frecuentes
- Configurar l铆mites de rate limiting
- Monitoreo con Prometheus/Grafana

##  Seguridad

- Implementar autenticaci贸n API key
- Configurar CORS apropiadamente
- Validar y sanitizar inputs
- Implementar rate limiting
- Logs de auditor铆a

##  Escalabilidad

- Usar m煤ltiples workers con Gunicorn
- Implementar queue system (Celery + Redis)
- Cache de respuestas frecuentes
- Auto-scaling basado en carga